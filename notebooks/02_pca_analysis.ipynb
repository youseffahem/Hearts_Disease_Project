{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:46:40.952939Z",
     "start_time": "2025-08-27T09:46:40.943496Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e554c2aabaab2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:46:43.361567Z",
     "start_time": "2025-08-27T09:46:41.071953Z"
    }
   },
   "outputs": [],
   "source": [
    "# Heart Disease UCI Dataset - PCA Analysis\n",
    "# Dimensionality Reduction using Principal Component Analysis\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Heart Disease Prediction - PCA Analysis ===\")\n",
    "print(\"Performing Principal Component Analysis for dimensionality reduction...\")\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    X_scaled = pd.read_csv('../data/X_scaled.csv')\n",
    "    y = pd.read_csv('../data/y.csv')['target']\n",
    "    print(\"✅ Preprocessed data loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Preprocessed data not found. Please run 01_data_preprocessing.ipynb first.\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 303, 13\n",
    "    X_scaled = pd.DataFrame(\n",
    "        np.random.randn(n_samples, n_features),\n",
    "        columns=[f'feature_{i}' for i in range(n_features)]\n",
    "    )\n",
    "    y = pd.Series(np.random.choice([0, 1], n_samples), name='target')\n",
    "    print(\"✅ Sample data created\")\n",
    "\n",
    "print(f\"\\nDataset shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {list(X_scaled.columns)}\")\n",
    "\n",
    "# 1. INITIAL PCA ANALYSIS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. INITIAL PCA ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply PCA with all components first\n",
    "pca_full = PCA()\n",
    "X_pca_full = pca_full.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Original number of features: {X_scaled.shape[1]}\")\n",
    "print(f\"Explained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(pca_full.explained_variance_ratio_):\n",
    "    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n",
    "\n",
    "# Cumulative explained variance\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "print(f\"\\nCumulative explained variance:\")\n",
    "for i, cum_var in enumerate(cumulative_variance):\n",
    "    print(f\"  First {i+1} PCs: {cum_var:.4f} ({cum_var*100:.2f}%)\")\n",
    "\n",
    "# 2. VISUALIZATION OF EXPLAINED VARIANCE\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. EXPLAINED VARIANCE VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create comprehensive PCA visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Principal Component Analysis - Heart Disease Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Individual explained variance\n",
    "axes[0,0].bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "              pca_full.explained_variance_ratio_)\n",
    "axes[0,0].set_xlabel('Principal Component')\n",
    "axes[0,0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0,0].set_title('Explained Variance by Component')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative explained variance\n",
    "axes[0,1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
    "               marker='o', linewidth=2, markersize=6)\n",
    "axes[0,1].axhline(y=0.95, color='red', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "axes[0,1].axhline(y=0.90, color='orange', linestyle='--', alpha=0.7, label='90% Variance')\n",
    "axes[0,1].axhline(y=0.85, color='green', linestyle='--', alpha=0.7, label='85% Variance')\n",
    "axes[0,1].set_xlabel('Number of Components')\n",
    "axes[0,1].set_ylabel('Cumulative Explained Variance')\n",
    "axes[0,1].set_title('Cumulative Explained Variance')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Scree plot\n",
    "axes[1,0].plot(range(1, len(pca_full.explained_variance_) + 1),\n",
    "               pca_full.explained_variance_, marker='o', linewidth=2, markersize=6)\n",
    "axes[1,0].set_xlabel('Principal Component')\n",
    "axes[1,0].set_ylabel('Eigenvalue')\n",
    "axes[1,0].set_title('Scree Plot')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# First two principal components scatter plot\n",
    "scatter = axes[1,1].scatter(X_pca_full[:, 0], X_pca_full[:, 1],\n",
    "                           c=y, cmap='coolwarm', alpha=0.7)\n",
    "axes[1,1].set_xlabel(f'PC1 ({pca_full.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[1,1].set_ylabel(f'PC2 ({pca_full.explained_variance_ratio_[1]:.2%} variance)')\n",
    "axes[1,1].set_title('First Two Principal Components')\n",
    "plt.colorbar(scatter, ax=axes[1,1], label='Heart Disease')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. DETERMINE OPTIMAL NUMBER OF COMPONENTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. DETERMINING OPTIMAL NUMBER OF COMPONENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Different criteria for selecting components\n",
    "criteria = {\n",
    "    '95% variance': 0.95,\n",
    "    '90% variance': 0.90,\n",
    "    '85% variance': 0.85\n",
    "}\n",
    "\n",
    "optimal_components = {}\n",
    "for criterion, threshold in criteria.items():\n",
    "    n_components = np.argmax(cumulative_variance >= threshold) + 1\n",
    "    optimal_components[criterion] = n_components\n",
    "    print(f\"📊 {criterion}: {n_components} components needed\")\n",
    "\n",
    "# Kaiser criterion (eigenvalue > 1)\n",
    "eigenvalues = pca_full.explained_variance_\n",
    "kaiser_components = np.sum(eigenvalues > 1)\n",
    "optimal_components['Kaiser criterion'] = kaiser_components\n",
    "print(f\"📊 Kaiser criterion (eigenvalue > 1): {kaiser_components} components\")\n",
    "\n",
    "# Elbow method visualization\n",
    "def find_elbow_point(variances):\n",
    "    \"\"\"Find elbow point using the rate of change method\"\"\"\n",
    "    n = len(variances)\n",
    "    if n < 3:\n",
    "        return 1\n",
    "\n",
    "    # Calculate second derivative (rate of change of rate of change)\n",
    "    second_derivatives = []\n",
    "    for i in range(1, n-1):\n",
    "        second_derivative = variances[i-1] - 2*variances[i] + variances[i+1]\n",
    "        second_derivatives.append(abs(second_derivative))\n",
    "\n",
    "    # Find the point with maximum second derivative\n",
    "    if second_derivatives:\n",
    "        elbow_index = np.argmax(second_derivatives) + 1  # +1 to adjust for indexing\n",
    "        return elbow_index + 1  # +1 because components start from 1\n",
    "    return 2\n",
    "\n",
    "elbow_components = find_elbow_point(pca_full.explained_variance_ratio_)\n",
    "optimal_components['Elbow method'] = elbow_components\n",
    "print(f\"📊 Elbow method: {elbow_components} components\")\n",
    "\n",
    "print(f\"\\n🎯 Summary of optimal components:\")\n",
    "for method, n_comp in optimal_components.items():\n",
    "    variance_explained = cumulative_variance[n_comp-1]\n",
    "    print(f\"  {method}: {n_comp} components ({variance_explained:.2%} variance)\")\n",
    "\n",
    "# 4. APPLY PCA WITH SELECTED NUMBER OF COMPONENTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. APPLYING PCA WITH OPTIMAL COMPONENTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the 90% variance criterion as default\n",
    "n_components_selected = optimal_components['90% variance']\n",
    "print(f\"🔧 Using {n_components_selected} principal components (90% variance criterion)\")\n",
    "\n",
    "# Apply PCA with selected components\n",
    "pca_selected = PCA(n_components=n_components_selected)\n",
    "X_pca_selected = pca_selected.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"✅ PCA applied successfully!\")\n",
    "print(f\"📊 Original dimensions: {X_scaled.shape}\")\n",
    "print(f\"📊 Reduced dimensions: {X_pca_selected.shape}\")\n",
    "print(f\"📊 Dimensionality reduction: {X_scaled.shape[1]} → {X_pca_selected.shape[1]}\")\n",
    "print(f\"📊 Variance retained: {np.sum(pca_selected.explained_variance_ratio_):.2%}\")\n",
    "\n",
    "# Create DataFrame with PCA components\n",
    "pca_columns = [f'PC{i+1}' for i in range(n_components_selected)]\n",
    "X_pca_df = pd.DataFrame(X_pca_selected, columns=pca_columns)\n",
    "\n",
    "print(f\"\\n📈 PCA components statistics:\")\n",
    "print(X_pca_df.describe())\n",
    "\n",
    "# 5. COMPONENT INTERPRETATION\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. PRINCIPAL COMPONENT INTERPRETATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Feature loadings (components)\n",
    "feature_names = X_scaled.columns\n",
    "loadings = pca_selected.components_\n",
    "\n",
    "print(\"📊 Feature loadings for principal components:\")\n",
    "loadings_df = pd.DataFrame(\n",
    "    loadings.T,\n",
    "    columns=[f'PC{i+1}' for i in range(n_components_selected)],\n",
    "    index=feature_names\n",
    ")\n",
    "print(loadings_df.round(3))\n",
    "\n",
    "# Visualize feature loadings\n",
    "fig, axes = plt.subplots(1, min(3, n_components_selected), figsize=(15, 5))\n",
    "if n_components_selected == 1:\n",
    "    axes = [axes]\n",
    "elif n_components_selected == 2:\n",
    "    axes = axes.flatten()\n",
    "\n",
    "for i in range(min(3, n_components_selected)):\n",
    "    loadings_sorted = loadings_df[f'PC{i+1}'].abs().sort_values(ascending=True)\n",
    "    colors = ['red' if x < 0 else 'blue' for x in loadings_df.loc[loadings_sorted.index, f'PC{i+1}']]\n",
    "\n",
    "    axes[i].barh(range(len(loadings_sorted)),\n",
    "                 loadings_df.loc[loadings_sorted.index, f'PC{i+1}'],\n",
    "                 color=colors, alpha=0.7)\n",
    "    axes[i].set_yticks(range(len(loadings_sorted)))\n",
    "    axes[i].set_yticklabels(loadings_sorted.index, fontsize=10)\n",
    "    axes[i].set_xlabel('Loading')\n",
    "    axes[i].set_title(f'PC{i+1} Feature Loadings\\n({pca_selected.explained_variance_ratio_[i]:.1%} variance)')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. PCA VISUALIZATION IN 2D AND 3D\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. PCA VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if n_components_selected >= 2:\n",
    "    # 2D visualization\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    scatter = plt.scatter(X_pca_selected[:, 0], X_pca_selected[:, 1],\n",
    "                         c=y, cmap='coolwarm', alpha=0.7, s=50)\n",
    "    plt.xlabel(f'PC1 ({pca_selected.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca_selected.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('Heart Disease Data in PC1-PC2 Space')\n",
    "    plt.colorbar(scatter, label='Heart Disease (0=No, 1=Yes)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    # Density plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for target_class in [0, 1]:\n",
    "        mask = y == target_class\n",
    "        plt.scatter(X_pca_selected[mask, 0], X_pca_selected[mask, 1],\n",
    "                   alpha=0.6, s=50, label=f'Class {target_class}')\n",
    "    plt.xlabel(f'PC1 ({pca_selected.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    plt.ylabel(f'PC2 ({pca_selected.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.title('Heart Disease Classes in PCA Space')\n",
    "    plt.legend(['No Disease', 'Disease'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if n_components_selected >= 3:\n",
    "    # 3D visualization\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    scatter = ax1.scatter(X_pca_selected[:, 0], X_pca_selected[:, 1], X_pca_selected[:, 2],\n",
    "                         c=y, cmap='coolwarm', alpha=0.6, s=50)\n",
    "    ax1.set_xlabel(f'PC1 ({pca_selected.explained_variance_ratio_[0]:.1%})')\n",
    "    ax1.set_ylabel(f'PC2 ({pca_selected.explained_variance_ratio_[1]:.1%})')\n",
    "    ax1.set_zlabel(f'PC3 ({pca_selected.explained_variance_ratio_[2]:.1%})')\n",
    "    ax1.set_title('3D PCA Visualization')\n",
    "\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    for target_class in [0, 1]:\n",
    "        mask = y == target_class\n",
    "        ax2.scatter(X_pca_selected[mask, 0], X_pca_selected[mask, 1], X_pca_selected[mask, 2],\n",
    "                   alpha=0.6, s=50, label=f'Class {target_class}')\n",
    "    ax2.set_xlabel(f'PC1 ({pca_selected.explained_variance_ratio_[0]:.1%})')\n",
    "    ax2.set_ylabel(f'PC2 ({pca_selected.explained_variance_ratio_[1]:.1%})')\n",
    "    ax2.set_zlabel(f'PC3 ({pca_selected.explained_variance_ratio_[2]:.1%})')\n",
    "    ax2.set_title('3D PCA by Class')\n",
    "    ax2.legend(['No Disease', 'Disease'])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 7. COMPARISON: ORIGINAL VS PCA DATA\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. ORIGINAL VS PCA DATA COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"📊 Data dimensionality comparison:\")\n",
    "print(f\"  Original data: {X_scaled.shape[1]} features\")\n",
    "print(f\"  PCA data: {X_pca_selected.shape[1]} components\")\n",
    "print(f\"  Reduction ratio: {(1 - X_pca_selected.shape[1]/X_scaled.shape[1]):.1%}\")\n",
    "\n",
    "print(f\"\\n📊 Variance retention:\")\n",
    "total_variance_retained = np.sum(pca_selected.explained_variance_ratio_)\n",
    "print(f\"  Variance retained: {total_variance_retained:.2%}\")\n",
    "print(f\"  Variance lost: {(1 - total_variance_retained):.2%}\")\n",
    "\n",
    "# Statistical comparison\n",
    "print(f\"\\n📊 Statistical properties:\")\n",
    "print(\"Original data:\")\n",
    "print(f\"  Mean: {X_scaled.mean().mean():.4f}\")\n",
    "print(f\"  Std: {X_scaled.std().mean():.4f}\")\n",
    "print(\"PCA data:\")\n",
    "print(f\"  Mean: {X_pca_df.mean().mean():.4f}\")\n",
    "print(f\"  Std: {X_pca_df.std().mean():.4f}\")\n",
    "\n",
    "# 8. SAVE PCA RESULTS\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"8. SAVING PCA RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "\n",
    "    # Save PCA transformed data\n",
    "    X_pca_df.to_csv('data/X_pca.csv', index=False)\n",
    "\n",
    "    # Save PCA model parameters\n",
    "    import joblib\n",
    "    joblib.dump(pca_selected, '../models/pca_model.pkl')\n",
    "\n",
    "    # Save PCA analysis results\n",
    "    pca_results = {\n",
    "        'n_components': n_components_selected,\n",
    "        'explained_variance_ratio': pca_selected.explained_variance_ratio_.tolist(),\n",
    "        'explained_variance': pca_selected.explained_variance_.tolist(),\n",
    "        'components': pca_selected.components_.tolist(),\n",
    "        'feature_names': feature_names.tolist(),\n",
    "        'total_variance_retained': float(total_variance_retained)\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open('../results/pca_analysis.json', 'w') as f:\n",
    "        json.dump(pca_results, f, indent=2)\n",
    "\n",
    "    # Save loadings matrix\n",
    "    loadings_df.to_csv('results/pca_loadings.csv')\n",
    "\n",
    "    print(\"✅ PCA results saved successfully!\")\n",
    "    print(\"Files saved:\")\n",
    "    print(\"  - X_pca.csv (PCA transformed data)\")\n",
    "    print(\"  - pca_model.pkl (PCA model)\")\n",
    "    print(\"  - pca_analysis.json (PCA results)\")\n",
    "    print(\"  - pca_loadings.csv (Feature loadings)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Error saving files: {e}\")\n",
    "    print(\"Creating necessary directories...\")\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# 9. PCA ANALYSIS SUMMARY\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"9. PCA ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"✅ PCA analysis completed successfully!\")\n",
    "print(f\"📊 Dimensionality reduction: {X_scaled.shape[1]} → {n_components_selected} features\")\n",
    "print(f\"📈 Variance retained: {total_variance_retained:.2%}\")\n",
    "print(f\"🎯 Selected components explain {total_variance_retained:.1%} of total variance\")\n",
    "\n",
    "print(f\"\\n📋 Top contributing features per component:\")\n",
    "for i in range(min(3, n_components_selected)):\n",
    "    pc_name = f'PC{i+1}'\n",
    "    top_features = loadings_df[pc_name].abs().nlargest(3)\n",
    "    print(f\"  {pc_name} ({pca_selected.explained_variance_ratio_[i]:.1%} variance):\")\n",
    "    for feature, loading in top_features.items():\n",
    "        direction = \"+\" if loadings_df.loc[feature, pc_name] > 0 else \"-\"\n",
    "        print(f\"    {direction} {feature} ({abs(loadings_df.loc[feature, pc_name]):.3f})\")\n",
    "\n",
    "print(f\"\\n🔍 Component interpretation guidelines:\")\n",
    "print(\"  - PC1: Usually captures the most significant pattern in data\")\n",
    "print(\"  - PC2: Captures the second most important orthogonal pattern\")\n",
    "print(\"  - Higher PCs: Capture remaining variance, often noise\")\n",
    "\n",
    "print(f\"\\n🎯 Next steps:\")\n",
    "print(\"  1. ✅ Data preprocessing complete\")\n",
    "print(\"  2. ✅ PCA analysis complete\")\n",
    "print(\"  3. ⏳ Feature selection (03_feature_selection.ipynb)\")\n",
    "print(\"  4. ⏳ Train supervised learning models (04_supervised_learning.ipynb)\")\n",
    "print(\"  5. ⏳ Apply unsupervised learning (05_unsupervised_learning.ipynb)\")\n",
    "print(\"  6. ⏳ Hyperparameter tuning (06_hyperparameter_tuning.ipynb)\")\n",
    "\n",
    "print(f\"\\n💡 Recommendations:\")\n",
    "print(f\"  - Use {n_components_selected} components for modeling to retain {total_variance_retained:.1%} variance\")\n",
    "print(\"  - Consider both original and PCA-transformed features for model comparison\")\n",
    "print(\"  - Monitor model performance difference between original and reduced data\")\n",
    "\n",
    "print(f\"\\n🎉 Ready to proceed to feature selection!\")\n",
    "\n",
    "# Display final PCA summary table\n",
    "print(f\"\\n📊 Final PCA Components Summary:\")\n",
    "summary_data = []\n",
    "for i in range(n_components_selected):\n",
    "    summary_data.append([\n",
    "        f'PC{i+1}',\n",
    "        f'{pca_selected.explained_variance_ratio_[i]:.3f}',\n",
    "        f'{pca_selected.explained_variance_ratio_[i]*100:.1f}%',\n",
    "        f'{cumulative_variance[i]*100:.1f}%'\n",
    "    ])\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data,\n",
    "                         columns=['Component', 'Variance Ratio', 'Individual %', 'Cumulative %'])\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
