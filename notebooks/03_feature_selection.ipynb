{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:47:01.057555Z",
     "start_time": "2025-08-27T09:46:51.713897Z"
    }
   },
   "outputs": [],
   "source": [
    "# Heart Disease UCI Dataset - Feature Selection\n",
    "# Statistical and ML-based Feature Selection Techniques\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, chi2, f_classif, mutual_info_classif,\n",
    "    RFE, RFECV, SelectFromModel\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Heart Disease Prediction - Feature Selection ===\")\n",
    "print(\"Applying statistical and ML-based feature selection techniques...\")\n",
    "\n",
    "# Load preprocessed data\n",
    "try:\n",
    "    X_scaled = pd.read_csv('../data/X_scaled.csv')\n",
    "    y = pd.read_csv('../data/y.csv')['target']\n",
    "    print(\"‚úÖ Preprocessed data loaded successfully\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Preprocessed data not found. Please run 01_data_preprocessing.ipynb first.\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples, n_features = 303, 13\n",
    "    feature_names = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n",
    "                    'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']\n",
    "    X_scaled = pd.DataFrame(\n",
    "        np.random.randn(n_samples, n_features),\n",
    "        columns=feature_names\n",
    "    )\n",
    "    y = pd.Series(np.random.choice([0, 1], n_samples), name='target')\n",
    "    print(\"‚úÖ Sample data created\")\n",
    "\n",
    "print(f\"\\nDataset shape: {X_scaled.shape}\")\n",
    "print(f\"Features: {list(X_scaled.columns)}\")\n",
    "print(f\"Target distribution: {dict(y.value_counts())}\")\n",
    "\n",
    "# 1. UNIVARIATE STATISTICAL TESTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. UNIVARIATE STATISTICAL TESTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Prepare data for statistical tests (ensure non-negative values for chi2)\n",
    "X_positive = MinMaxScaler().fit_transform(X_scaled)\n",
    "X_positive = pd.DataFrame(X_positive, columns=X_scaled.columns)\n",
    "\n",
    "# Chi-square test\n",
    "print(\"üîç Chi-square Test (for categorical/discrete features):\")\n",
    "chi2_selector = SelectKBest(chi2, k='all')\n",
    "chi2_selector.fit(X_positive, y)\n",
    "chi2_scores = chi2_selector.scores_\n",
    "chi2_pvalues = chi2_selector.pvalues_\n",
    "\n",
    "chi2_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'Chi2_Score': chi2_scores,\n",
    "    'Chi2_P_value': chi2_pvalues\n",
    "}).sort_values('Chi2_Score', ascending=False)\n",
    "\n",
    "print(chi2_results.round(4))\n",
    "\n",
    "# F-test (ANOVA)\n",
    "print(f\"\\nüîç F-test (ANOVA) - for continuous features:\")\n",
    "f_selector = SelectKBest(f_classif, k='all')\n",
    "f_selector.fit(X_scaled, y)\n",
    "f_scores = f_selector.scores_\n",
    "f_pvalues = f_selector.pvalues_\n",
    "\n",
    "f_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'F_Score': f_scores,\n",
    "    'F_P_value': f_pvalues\n",
    "}).sort_values('F_Score', ascending=False)\n",
    "\n",
    "print(f_results.round(4))\n",
    "\n",
    "# Mutual Information\n",
    "print(f\"\\nüîç Mutual Information:\")\n",
    "mi_scores = mutual_info_classif(X_scaled, y, random_state=42)\n",
    "mi_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'MI_Score': mi_scores\n",
    "}).sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(mi_results.round(4))\n",
    "\n",
    "# 2. VISUALIZE STATISTICAL TESTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. STATISTICAL TESTS VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Feature Selection - Statistical Tests', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Chi-square scores\n",
    "chi2_sorted = chi2_results.sort_values('Chi2_Score', ascending=True)\n",
    "axes[0,0].barh(range(len(chi2_sorted)), chi2_sorted['Chi2_Score'], alpha=0.7)\n",
    "axes[0,0].set_yticks(range(len(chi2_sorted)))\n",
    "axes[0,0].set_yticklabels(chi2_sorted['Feature'])\n",
    "axes[0,0].set_xlabel('Chi-square Score')\n",
    "axes[0,0].set_title('Chi-square Test Scores')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# F-test scores\n",
    "f_sorted = f_results.sort_values('F_Score', ascending=True)\n",
    "axes[0,1].barh(range(len(f_sorted)), f_sorted['F_Score'], alpha=0.7, color='orange')\n",
    "axes[0,1].set_yticks(range(len(f_sorted)))\n",
    "axes[0,1].set_yticklabels(f_sorted['Feature'])\n",
    "axes[0,1].set_xlabel('F-test Score')\n",
    "axes[0,1].set_title('F-test (ANOVA) Scores')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Mutual Information scores\n",
    "mi_sorted = mi_results.sort_values('MI_Score', ascending=True)\n",
    "axes[1,0].barh(range(len(mi_sorted)), mi_sorted['MI_Score'], alpha=0.7, color='green')\n",
    "axes[1,0].set_yticks(range(len(mi_sorted)))\n",
    "axes[1,0].set_yticklabels(mi_sorted['Feature'])\n",
    "axes[1,0].set_xlabel('Mutual Information Score')\n",
    "axes[1,0].set_title('Mutual Information Scores')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# P-values comparison\n",
    "p_value_data = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'Chi2_P_value': chi2_pvalues,\n",
    "    'F_P_value': f_pvalues\n",
    "})\n",
    "p_value_melted = p_value_data.melt(id_vars=['Feature'], var_name='Test', value_name='P_value')\n",
    "sns.barplot(data=p_value_melted, x='P_value', y='Feature', hue='Test', ax=axes[1,1])\n",
    "axes[1,1].axvline(x=0.05, color='red', linestyle='--', alpha=0.7, label='Œ±=0.05')\n",
    "axes[1,1].set_xlabel('P-value')\n",
    "axes[1,1].set_title('Statistical Significance (P-values)')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. MACHINE LEARNING-BASED FEATURE SELECTION\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. MACHINE LEARNING-BASED FEATURE SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Random Forest Feature Importance\n",
    "print(\"üå≥ Random Forest Feature Importance:\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_scaled, y)\n",
    "rf_importance = rf.feature_importances_\n",
    "\n",
    "rf_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'RF_Importance': rf_importance\n",
    "}).sort_values('RF_Importance', ascending=False)\n",
    "\n",
    "print(rf_results.round(4))\n",
    "\n",
    "# XGBoost Feature Importance\n",
    "print(f\"\\nüöÄ XGBoost Feature Importance:\")\n",
    "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_scaled, y)\n",
    "xgb_importance = xgb_model.feature_importances_\n",
    "\n",
    "xgb_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'XGB_Importance': xgb_importance\n",
    "}).sort_values('XGB_Importance', ascending=False)\n",
    "\n",
    "print(xgb_results.round(4))\n",
    "\n",
    "# Logistic Regression Coefficients\n",
    "print(f\"\\nüìä Logistic Regression Coefficients (L1 regularization):\")\n",
    "lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', random_state=42)\n",
    "lr_l1.fit(X_scaled, y)\n",
    "lr_coef = np.abs(lr_l1.coef_[0])\n",
    "\n",
    "lr_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'LR_Coef_Abs': lr_coef\n",
    "}).sort_values('LR_Coef_Abs', ascending=False)\n",
    "\n",
    "print(lr_results.round(4))\n",
    "\n",
    "# 4. VISUALIZE ML-BASED FEATURE IMPORTANCE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. ML-BASED FEATURE IMPORTANCE VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Machine Learning-based Feature Importance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Random Forest importance\n",
    "rf_sorted = rf_results.sort_values('RF_Importance', ascending=True)\n",
    "axes[0].barh(range(len(rf_sorted)), rf_sorted['RF_Importance'], alpha=0.7, color='green')\n",
    "axes[0].set_yticks(range(len(rf_sorted)))\n",
    "axes[0].set_yticklabels(rf_sorted['Feature'])\n",
    "axes[0].set_xlabel('Importance Score')\n",
    "axes[0].set_title('Random Forest Feature Importance')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# XGBoost importance\n",
    "xgb_sorted = xgb_results.sort_values('XGB_Importance', ascending=True)\n",
    "axes[1].barh(range(len(xgb_sorted)), xgb_sorted['XGB_Importance'], alpha=0.7, color='purple')\n",
    "axes[1].set_yticks(range(len(xgb_sorted)))\n",
    "axes[1].set_yticklabels(xgb_sorted['Feature'])\n",
    "axes[1].set_xlabel('Importance Score')\n",
    "axes[1].set_title('XGBoost Feature Importance')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "lr_sorted = lr_results.sort_values('LR_Coef_Abs', ascending=True)\n",
    "axes[2].barh(range(len(lr_sorted)), lr_sorted['LR_Coef_Abs'], alpha=0.7, color='red')\n",
    "axes[2].set_yticks(range(len(lr_sorted)))\n",
    "axes[2].set_yticklabels(lr_sorted['Feature'])\n",
    "axes[2].set_xlabel('Absolute Coefficient')\n",
    "axes[2].set_title('Logistic Regression Coefficients')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. RECURSIVE FEATURE ELIMINATION (RFE)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. RECURSIVE FEATURE ELIMINATION (RFE)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# RFE with Random Forest\n",
    "print(\"üîÑ RFE with Random Forest:\")\n",
    "rfe_rf = RFE(estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "             n_features_to_select=8)\n",
    "rfe_rf.fit(X_scaled, y)\n",
    "\n",
    "rfe_rf_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'RFE_RF_Selected': rfe_rf.support_,\n",
    "    'RFE_RF_Ranking': rfe_rf.ranking_\n",
    "}).sort_values('RFE_RF_Ranking')\n",
    "\n",
    "print(rfe_rf_results)\n",
    "\n",
    "# RFE with Logistic Regression\n",
    "print(f\"\\nüîÑ RFE with Logistic Regression:\")\n",
    "rfe_lr = RFE(estimator=LogisticRegression(random_state=42),\n",
    "             n_features_to_select=8)\n",
    "rfe_lr.fit(X_scaled, y)\n",
    "\n",
    "rfe_lr_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'RFE_LR_Selected': rfe_lr.support_,\n",
    "    'RFE_LR_Ranking': rfe_lr.ranking_\n",
    "}).sort_values('RFE_LR_Ranking')\n",
    "\n",
    "print(rfe_lr_results)\n",
    "\n",
    "# RFE with Cross-Validation\n",
    "print(f\"\\nüîÑ RFE with Cross-Validation (Random Forest):\")\n",
    "rfecv = RFECV(estimator=RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "              step=1, cv=5, scoring='accuracy')\n",
    "rfecv.fit(X_scaled, y)\n",
    "\n",
    "print(f\"Optimal number of features: {rfecv.n_features_}\")\n",
    "print(f\"Cross-validation scores: {rfecv.cv_results_['mean_test_score']}\")\n",
    "\n",
    "rfecv_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'RFECV_Selected': rfecv.support_,\n",
    "    'RFECV_Ranking': rfecv.ranking_\n",
    "}).sort_values('RFECV_Ranking')\n",
    "\n",
    "print(rfecv_results)\n",
    "\n",
    "# Plot RFECV results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1),\n",
    "         rfecv.cv_results_['mean_test_score'], marker='o', linewidth=2, markersize=6)\n",
    "plt.xlabel('Number of Features Selected')\n",
    "plt.ylabel('Cross-Validation Accuracy')\n",
    "plt.title('Recursive Feature Elimination with Cross-Validation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=rfecv.n_features_, color='red', linestyle='--',\n",
    "            label=f'Optimal: {rfecv.n_features_} features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. FEATURE SELECTION BASED ON MODEL PERFORMANCE\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"6. MODEL-BASED FEATURE SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# SelectFromModel with Random Forest\n",
    "print(\"üéØ SelectFromModel with Random Forest (median threshold):\")\n",
    "sfm_rf = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                        threshold='median')\n",
    "sfm_rf.fit(X_scaled, y)\n",
    "\n",
    "sfm_rf_selected = X_scaled.columns[sfm_rf.get_support()]\n",
    "print(f\"Selected features ({len(sfm_rf_selected)}): {list(sfm_rf_selected)}\")\n",
    "\n",
    "# SelectFromModel with L1 Logistic Regression\n",
    "print(f\"\\nüéØ SelectFromModel with L1 Logistic Regression:\")\n",
    "sfm_lr = SelectFromModel(LogisticRegression(penalty='l1', solver='liblinear', random_state=42))\n",
    "sfm_lr.fit(X_scaled, y)\n",
    "\n",
    "sfm_lr_selected = X_scaled.columns[sfm_lr.get_support()]\n",
    "print(f\"Selected features ({len(sfm_lr_selected)}): {list(sfm_lr_selected)}\")\n",
    "\n",
    "# 7. COMBINE ALL FEATURE SELECTION RESULTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"7. COMPREHENSIVE FEATURE SELECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create comprehensive results dataframe\n",
    "comprehensive_results = pd.DataFrame({\n",
    "    'Feature': X_scaled.columns,\n",
    "    'Chi2_Score': chi2_results.set_index('Feature').loc[X_scaled.columns, 'Chi2_Score'],\n",
    "    'F_Score': f_results.set_index('Feature').loc[X_scaled.columns, 'F_Score'],\n",
    "    'MI_Score': mi_results.set_index('Feature').loc[X_scaled.columns, 'MI_Score'],\n",
    "    'RF_Importance': rf_results.set_index('Feature').loc[X_scaled.columns, 'RF_Importance'],\n",
    "    'XGB_Importance': xgb_results.set_index('Feature').loc[X_scaled.columns, 'XGB_Importance'],\n",
    "    'LR_Coef_Abs': lr_results.set_index('Feature').loc[X_scaled.columns, 'LR_Coef_Abs'],\n",
    "    'RFE_RF_Ranking': rfe_rf_results.set_index('Feature').loc[X_scaled.columns, 'RFE_RF_Ranking'],\n",
    "    'RFE_LR_Ranking': rfe_lr_results.set_index('Feature').loc[X_scaled.columns, 'RFE_LR_Ranking'],\n",
    "    'RFECV_Ranking': rfecv_results.set_index('Feature').loc[X_scaled.columns, 'RFECV_Ranking']\n",
    "})\n",
    "\n",
    "# Normalize scores for comparison (0-1 scale)\n",
    "score_columns = ['Chi2_Score', 'F_Score', 'MI_Score', 'RF_Importance', 'XGB_Importance', 'LR_Coef_Abs']\n",
    "for col in score_columns:\n",
    "    comprehensive_results[f'{col}_Normalized'] = (\n",
    "        comprehensive_results[col] / comprehensive_results[col].max()\n",
    "    )\n",
    "\n",
    "# Calculate combined score\n",
    "normalized_cols = [col for col in comprehensive_results.columns if '_Normalized' in col]\n",
    "comprehensive_results['Combined_Score'] = comprehensive_results[normalized_cols].mean(axis=1)\n",
    "\n",
    "# Rank features\n",
    "comprehensive_results['Final_Ranking'] = comprehensive_results['Combined_Score'].rank(ascending=False)\n",
    "comprehensive_results = comprehensive_results.sort_values('Final_Ranking')\n",
    "\n",
    "print(\"üìä Comprehensive Feature Selection Results:\")\n",
    "print(comprehensive_results.round(3))\n",
    "\n",
    "# 8. VISUALIZE COMPREHENSIVE RESULTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"8. COMPREHENSIVE RESULTS VISUALIZATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Feature selection heatmap\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Normalized scores heatmap\n",
    "scores_for_heatmap = comprehensive_results.set_index('Feature')[normalized_cols]\n",
    "sns.heatmap(scores_for_heatmap.T, annot=True, cmap='YlOrRd', ax=ax1,\n",
    "            cbar_kws={'label': 'Normalized Score'})\n",
    "ax1.set_title('Feature Selection Methods Comparison (Normalized Scores)')\n",
    "ax1.set_ylabel('Selection Method')\n",
    "\n",
    "# Combined scores\n",
    "comprehensive_sorted = comprehensive_results.sort_values('Combined_Score', ascending=True)\n",
    "ax2.barh(range(len(comprehensive_sorted)), comprehensive_sorted['Combined_Score'], alpha=0.7)\n",
    "ax2.set_yticks(range(len(comprehensive_sorted)))\n",
    "ax2.set_yticklabels(comprehensive_sorted['Feature'])\n",
    "ax2.set_xlabel('Combined Score')\n",
    "ax2.set_title('Final Feature Ranking (Combined Score)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 9. SELECT TOP FEATURES AND CREATE REDUCED DATASETS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"9. FINAL FEATURE SELECTION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Select top features based on different criteria\n",
    "top_k_features = 8  # Select top 8 features\n",
    "\n",
    "# Method 1: Top combined score features\n",
    "top_combined_features = comprehensive_results.head(top_k_features)['Feature'].tolist()\n",
    "print(f\"üèÜ Top {top_k_features} features (Combined Score):\")\n",
    "for i, feature in enumerate(top_combined_features, 1):\n",
    "    score = comprehensive_results[comprehensive_results['Feature'] == feature]['Combined_Score'].iloc[0]\n",
    "    print(f\"  {i}. {feature} (score: {score:.3f})\")\n",
    "\n",
    "# Method 2: RFECV selected features\n",
    "rfecv_selected_features = rfecv_results[rfecv_results['RFECV_Selected']]['Feature'].tolist()\n",
    "print(f\"\\nüîÑ RFECV Selected features ({len(rfecv_selected_features)}):\")\n",
    "for i, feature in enumerate(rfecv_selected_features, 1):\n",
    "    print(f\"  {i}. {feature}\")\n",
    "\n",
    "# Method 3: Consensus features (selected by multiple methods)\n",
    "selection_methods = ['RFE_RF_Selected', 'RFE_LR_Selected', 'RFECV_Selected']\n",
    "if len(sfm_rf_selected) > 0:\n",
    "    comprehensive_results['SFM_RF_Selected'] = comprehensive_results['Feature'].isin(sfm_rf_selected)\n",
    "    selection_methods.append('SFM_RF_Selected')\n",
    "if len(sfm_lr_selected) > 0:\n",
    "    comprehensive_results['SFM_LR_Selected'] = comprehensive_results['Feature'].isin(sfm_lr_selected)\n",
    "    selection_methods.append('SFM_LR_Selected')\n",
    "\n",
    "# Add RFE selections to comprehensive results\n",
    "comprehensive_results = comprehensive_results.merge(\n",
    "    rfe_rf_results[['Feature', 'RFE_RF_Selected']], on='Feature'\n",
    ").merge(\n",
    "    rfe_lr_results[['Feature', 'RFE_LR_Selected']], on='Feature'\n",
    ").merge(\n",
    "    rfecv_results[['Feature', 'RFECV_Selected']], on='Feature'\n",
    ")\n",
    "\n",
    "# Count selections\n",
    "available_methods = [col for col in comprehensive_results.columns if col in selection_methods]\n",
    "comprehensive_results['Selection_Count'] = comprehensive_results[available_methods].sum(axis=1)\n",
    "consensus_features = comprehensive_results[comprehensive_results['Selection_Count'] >= 2].sort_values('Selection_Count', ascending=False)\n",
    "\n",
    "print(f\"\\nü§ù Consensus features (selected by ‚â•2 methods):\")\n",
    "for _, row in consensus_features.iterrows():\n",
    "    print(f\"  {row['Feature']} (selected by {int(row['Selection_Count'])} methods)\")\n",
    "\n",
    "# Create reduced datasets\n",
    "print(f\"\\nüìä Creating reduced datasets...\")\n",
    "\n",
    "# Dataset 1: Top combined score features\n",
    "X_top_combined = X_scaled[top_combined_features]\n",
    "\n",
    "# Dataset 2: RFECV features\n",
    "X_rfecv = X_scaled[rfecv_selected_features]\n",
    "\n",
    "# Dataset 3: Consensus features (if any)\n",
    "if len(consensus_features) > 0:\n",
    "    consensus_feature_names = consensus_features['Feature'].tolist()\n",
    "    X_consensus = X_scaled[consensus_feature_names]\n",
    "else:\n",
    "    X_consensus = X_scaled[top_combined_features[:6]]  # Fallback\n",
    "    consensus_feature_names = top_combined_features[:6]\n",
    "\n",
    "print(f\"‚úÖ Reduced datasets created:\")\n",
    "print(f\"  - Top Combined: {X_top_combined.shape[1]} features\")\n",
    "print(f\"  - RFECV: {X_rfecv.shape[1]} features\")\n",
    "print(f\"  - Consensus: {X_consensus.shape[1]} features\")\n",
    "\n",
    "# 10. SAVE FEATURE SELECTION RESULTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"10. SAVING FEATURE SELECTION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    import os\n",
    "    os.makedirs('../data', exist_ok=True)\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "    # Save reduced datasets\n",
    "    X_top_combined.to_csv('data/X_top_features.csv', index=False)\n",
    "    X_rfecv.to_csv('data/X_rfecv_features.csv', index=False)\n",
    "    X_consensus.to_csv('data/X_consensus_features.csv', index=False)\n",
    "\n",
    "    # Save feature selection results\n",
    "    comprehensive_results.to_csv('results/feature_selection_results.csv', index=False)\n",
    "\n",
    "    # Save feature lists\n",
    "    feature_selections = {\n",
    "        'top_combined_features': top_combined_features,\n",
    "        'rfecv_selected_features': rfecv_selected_features,\n",
    "        'consensus_features': consensus_feature_names,\n",
    "        'all_features': X_scaled.columns.tolist()\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open('../results/selected_features.json', 'w') as f:\n",
    "        json.dump(feature_selections, f, indent=2)\n",
    "\n",
    "    print(\"‚úÖ Feature selection results saved successfully!\")\n",
    "    print(\"Files saved:\")\n",
    "    print(\"  - X_top_features.csv (Top combined score features)\")\n",
    "    print(\"  - X_rfecv_features.csv (RFECV selected features)\")\n",
    "    print(\"  - X_consensus_features.csv (Consensus features)\")\n",
    "    print(\"  - feature_selection_results.csv (Comprehensive results)\")\n",
    "    print(\"  - selected_features.json (Feature lists)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error saving files: {e}\")\n",
    "\n",
    "# 11. FEATURE SELECTION SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"11. FEATURE SELECTION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Feature selection analysis completed successfully!\")\n",
    "print(f\"üìä Original features: {X_scaled.shape[1]}\")\n",
    "print(f\"üìä Selected features (Combined): {len(top_combined_features)}\")\n",
    "print(f\"üìä Selected features (RFECV): {len(rfecv_selected_features)}\")\n",
    "print(f\"üìä Selected features (Consensus): {len(consensus_feature_names)}\")\n",
    "\n",
    "print(f\"\\nüèÜ Top 5 most important features (Combined Score):\")\n",
    "for i, (_, row) in enumerate(comprehensive_results.head(5).iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Feature']} (score: {row['Combined_Score']:.3f})\")\n",
    "\n",
    "print(f\"\\nüîç Feature selection methods used:\")\n",
    "print(\"  ‚úì Chi-square test\")\n",
    "print(\"  ‚úì F-test (ANOVA)\")\n",
    "print(\"  ‚úì Mutual Information\")\n",
    "print(\"  ‚úì Random Forest importance\")\n",
    "print(\"  ‚úì XGBoost importance\")\n",
    "print(\"  ‚úì Logistic Regression coefficients\")\n",
    "print(\"  ‚úì Recursive Feature Elimination (RFE)\")\n",
    "print(\"  ‚úì RFE with Cross-Validation (RFECV)\")\n",
    "print(\"  ‚úì SelectFromModel\")\n",
    "\n",
    "print(f\"\\nüí° Recommendations:\")\n",
    "print(\"  - Use top combined features for initial modeling\")\n",
    "print(\"  - Compare performance with RFECV features\")\n",
    "print(\"  - Consider consensus features for robust model\")\n",
    "print(\"  - Monitor overfitting with reduced feature sets\")\n",
    "\n",
    "print(f\"\\nüéØ Next steps:\")\n",
    "print(\"  1. ‚úÖ Data preprocessing complete\")\n",
    "print(\"  2. ‚úÖ PCA analysis complete\")\n",
    "print(\"  3. ‚úÖ Feature selection complete\")\n",
    "print(\"  4. ‚è≥ Train supervised learning models (04_supervised_learning.ipynb)\")\n",
    "print(\"  5. ‚è≥ Apply unsupervised learning (05_unsupervised_learning.ipynb)\")\n",
    "print(\"  6. ‚è≥ Hyperparameter tuning (06_hyperparameter_tuning.ipynb)\")\n",
    "\n",
    "print(f\"\\nüéâ Ready to proceed to supervised learning models!\")\n",
    "\n",
    "# Display feature comparison table\n",
    "print(f\"\\nüìä Feature Selection Comparison:\")\n",
    "comparison_data = []\n",
    "for feature in X_scaled.columns:\n",
    "    row_data = comprehensive_results[comprehensive_results['Feature'] == feature]\n",
    "    if len(row_data) > 0:\n",
    "        comparison_data.append([\n",
    "            feature,\n",
    "            '‚úì' if feature in top_combined_features else '‚úó',\n",
    "            '‚úì' if feature in rfecv_selected_features else '‚úó',\n",
    "            '‚úì' if feature in consensus_feature_names else '‚úó',\n",
    "            f\"{row_data['Combined_Score'].iloc[0]:.3f}\"\n",
    "        ])\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data,\n",
    "                           columns=['Feature', 'Top Combined', 'RFECV', 'Consensus', 'Score'])\n",
    "print(comparison_df.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
