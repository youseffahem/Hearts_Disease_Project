{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-27T09:51:33.276769Z",
     "start_time": "2025-08-27T09:47:35.423997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Heart Disease UCI Dataset - Hyperparameter Tuning\n",
    "# GridSearchCV and RandomizedSearchCV for Model Optimization\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV, RandomizedSearchCV, StratifiedKFold,\n",
    "    validation_curve, learning_curve\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "import joblib\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from scipy.stats import randint, uniform\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"=== Heart Disease Prediction - Hyperparameter Tuning ===\")\n",
    "print(\"Optimizing model performance using GridSearchCV and RandomizedSearchCV...\")\n",
    "\n",
    "# Load preprocessed data and previous results\n",
    "try:\n",
    "    X_scaled = pd.read_csv('../data/X_scaled.csv')\n",
    "    X_top_features = pd.read_csv('../data/X_top_features.csv')\n",
    "    y = pd.read_csv('../data/y.csv')['target']\n",
    "\n",
    "    # Load train-test splits\n",
    "    X_train = pd.read_csv('../data/X_train.csv')\n",
    "    X_test = pd.read_csv('../data/X_test.csv')\n",
    "    y_train = pd.read_csv('../data/y_train.csv')['target']\n",
    "    y_test = pd.read_csv('../data/y_test.csv')['target']\n",
    "\n",
    "    print(\"‚úÖ Data loaded successfully\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Data files not found. Please run previous notebooks first.\")\n",
    "    # Create sample data for demonstration\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 303\n",
    "    X_scaled = pd.DataFrame(\n",
    "        np.random.randn(n_samples, 13),\n",
    "        columns=[f'feature_{i}' for i in range(13)]\n",
    "    )\n",
    "    X_top_features = X_scaled.iloc[:, :8]\n",
    "    y = pd.Series(np.random.choice([0, 1], n_samples), name='target')\n",
    "\n",
    "    # Create train-test split\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    print(\"‚úÖ Sample data created\")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Target distribution: {dict(y.value_counts())}\")\n",
    "\n",
    "# Prepare datasets for tuning\n",
    "datasets = {\n",
    "    'Original': {'X_train': X_train, 'X_test': X_test},\n",
    "    'Top_Features': {\n",
    "        'X_train': X_train[X_top_features.columns] if set(X_top_features.columns).issubset(X_train.columns) else X_train.iloc[:, :8],\n",
    "        'X_test': X_test[X_top_features.columns] if set(X_top_features.columns).issubset(X_test.columns) else X_test.iloc[:, :8]\n",
    "    }\n",
    "}\n",
    "\n",
    "# 1. DEFINE HYPERPARAMETER GRIDS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"1. DEFINING HYPERPARAMETER SEARCH SPACES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Define parameter grids for GridSearchCV\n",
    "param_grids = {\n",
    "    'LogisticRegression': {\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear'],\n",
    "        'max_iter': [1000]\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': [3, 5, 7, 10, None],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 5, 10],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [3, 5, 7, 10, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 5],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define parameter distributions for RandomizedSearchCV\n",
    "param_distributions = {\n",
    "    'LogisticRegression': {\n",
    "        'C': uniform(0.001, 100),\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'DecisionTree': {\n",
    "        'max_depth': randint(3, 21),\n",
    "        'min_samples_split': randint(2, 21),\n",
    "        'min_samples_leaf': randint(1, 11),\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': randint(50, 301),\n",
    "        'max_depth': randint(3, 21),\n",
    "        'min_samples_split': randint(2, 21),\n",
    "        'min_samples_leaf': randint(1, 11),\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'SVM': {\n",
    "        'C': uniform(0.1, 100),\n",
    "        'kernel': ['linear', 'rbf', 'poly'],\n",
    "        'gamma': ['scale', 'auto'] + list(uniform(0.001, 1).rvs(5))\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üîß Parameter grids defined for:\")\n",
    "for model_name in param_grids.keys():\n",
    "    print(f\"  - {model_name}: {len(param_grids[model_name])} parameters\")\n",
    "\n",
    "# 2. GRID SEARCH CV\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"2. GRID SEARCH CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'DecisionTree': DecisionTreeClassifier(random_state=42),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scoring = 'f1'\n",
    "\n",
    "# Store results\n",
    "grid_search_results = {}\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\nüìä Grid Search on {dataset_name} dataset...\")\n",
    "    grid_search_results[dataset_name] = {}\n",
    "    X_train_data = data['X_train']\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  üîç Tuning {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model,\n",
    "            param_grid=param_grids[model_name],\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0\n",
    "        )\n",
    "        grid_search.fit(X_train_data, y_train)\n",
    "        end_time = time.time()\n",
    "        grid_search_results[dataset_name][model_name] = {\n",
    "            'best_estimator': grid_search.best_estimator_,\n",
    "            'best_params': grid_search.best_params_,\n",
    "            'best_score': grid_search.best_score_,\n",
    "            'cv_results': grid_search.cv_results_,\n",
    "            'grid_search_time': end_time - start_time\n",
    "        }\n",
    "        print(f\"    Best score: {grid_search.best_score_:.4f}\")\n",
    "        print(f\"    Best params: {grid_search.best_params_}\")\n",
    "        print(f\"    Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 3. RANDOMIZED SEARCH CV\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"3. RANDOMIZED SEARCH CROSS-VALIDATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "random_search_results = {}\n",
    "n_iter = 50\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\nüé≤ Randomized Search on {dataset_name} dataset...\")\n",
    "    random_search_results[dataset_name] = {}\n",
    "    X_train_data = data['X_train']\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"  üîç Tuning {model_name}...\")\n",
    "        start_time = time.time()\n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_distributions[model_name],\n",
    "            n_iter=n_iter,\n",
    "            cv=cv,\n",
    "            scoring=scoring,\n",
    "            n_jobs=-1,\n",
    "            verbose=0,\n",
    "            random_state=42\n",
    "        )\n",
    "        random_search.fit(X_train_data, y_train)\n",
    "        end_time = time.time()\n",
    "        random_search_results[dataset_name][model_name] = {\n",
    "            'best_estimator': random_search.best_estimator_,\n",
    "            'best_params': random_search.best_params_,\n",
    "            'best_score': random_search.best_score_,\n",
    "            'cv_results': random_search.cv_results_,\n",
    "            'random_search_time': end_time - start_time\n",
    "        }\n",
    "        print(f\"    Best score: {random_search.best_score_:.4f}\")\n",
    "        print(f\"    Best params: {random_search.best_params_}\")\n",
    "        print(f\"    Time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# 4. COMPARE OPTIMIZATION METHODS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"4. OPTIMIZATION METHODS COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "comparison_data = []\n",
    "for dataset_name in datasets.keys():\n",
    "    for model_name in models.keys():\n",
    "        grid_result = grid_search_results[dataset_name][model_name]\n",
    "        comparison_data.append([\n",
    "            dataset_name, model_name, 'GridSearchCV',\n",
    "            grid_result['best_score'],\n",
    "            grid_result['grid_search_time']\n",
    "        ])\n",
    "        random_result = random_search_results[dataset_name][model_name]\n",
    "        comparison_data.append([\n",
    "            dataset_name, model_name, 'RandomizedSearchCV',\n",
    "            random_result['best_score'],\n",
    "            random_result['random_search_time']\n",
    "        ])\n",
    "comparison_df = pd.DataFrame(comparison_data,\n",
    "                           columns=['Dataset', 'Model', 'Method', 'Best_Score', 'Time'])\n",
    "print(\"üìä Optimization Methods Comparison:\")\n",
    "print(comparison_df.round(4))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "score_pivot = comparison_df.pivot_table(index=['Dataset', 'Model'],\n",
    "                                       columns='Method',\n",
    "                                       values='Best_Score')\n",
    "score_pivot.plot(kind='bar', ax=axes[0], alpha=0.8)\n",
    "axes[0].set_title('Best Cross-Validation Scores')\n",
    "axes[0].set_ylabel('F1 Score')\n",
    "axes[0].legend(title='Method')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "plt.setp(axes[0].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "time_pivot = comparison_df.pivot_table(index=['Dataset', 'Model'],\n",
    "                                      columns='Method',\n",
    "                                      values='Time')\n",
    "time_pivot.plot(kind='bar', ax=axes[1], alpha=0.8)\n",
    "axes[1].set_title('Optimization Time Comparison')\n",
    "axes[1].set_ylabel('Time (seconds)')\n",
    "axes[1].legend(title='Method')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "plt.setp(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. EVALUATE BEST MODELS ON TEST SET\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"5. TEST SET EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_results = {}\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\nüìä Test Set Evaluation - {dataset_name} Dataset:\")\n",
    "    test_results[dataset_name] = {}\n",
    "    X_test_data = data['X_test']\n",
    "\n",
    "    for model_name in models.keys():\n",
    "        best_model = grid_search_results[dataset_name][model_name]['best_estimator']\n",
    "        y_pred = best_model.predict(X_test_data)\n",
    "        y_pred_proba = best_model.predict_proba(X_test_data)[:, 1]\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "        test_results[dataset_name][model_name] = {\n",
    "            'accuracy': accuracy,\n",
    "            'f1_score': f1,\n",
    "            'roc_auc': roc_auc,\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        print(f\"  {model_name}:\")\n",
    "        print(f\"    Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"    F1 Score: {f1:.4f}\")\n",
    "        print(f\"    ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# 6. LEARNING CURVES\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"6. LEARNING CURVES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def plot_learning_curves(estimator, X, y, title, cv=None):\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='f1', random_state=42\n",
    "    )\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation score')\n",
    "    plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'Learning Curves - {title}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"üìà Generating learning curves for best models...\")\n",
    "for dataset_name, data in datasets.items():\n",
    "    X_train_data = data['X_train']\n",
    "    best_f1 = 0\n",
    "    best_model_name = None\n",
    "    for model_name in models.keys():\n",
    "        f1 = grid_search_results[dataset_name][model_name]['best_score']\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_model_name = model_name\n",
    "    if best_model_name:\n",
    "        best_estimator = grid_search_results[dataset_name][best_model_name]['best_estimator']\n",
    "        plot_learning_curves(best_estimator, X_train_data, y_train,\n",
    "                           f'{best_model_name} - {dataset_name}', cv=cv)\n",
    "\n",
    "# 7. VALIDATION CURVES\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"7. VALIDATION CURVES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# ===================================================================\n",
    "# START OF THE CORRECTED CODE BLOCK\n",
    "# ===================================================================\n",
    "def plot_validation_curve(estimator, X, y, param_name, param_range, title):\n",
    "    \"\"\"Plot validation curve for a specific parameter\"\"\"\n",
    "    # THE FIX IS HERE: using keyword arguments for param_name and param_range\n",
    "    train_scores, val_scores = validation_curve(\n",
    "        estimator, X, y,\n",
    "        param_name=param_name,\n",
    "        param_range=param_range,\n",
    "        cv=cv, scoring='f1', n_jobs=-1\n",
    "    )\n",
    "\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(param_range, train_mean, 'o-', color='blue', label='Training score')\n",
    "    plt.fill_between(param_range, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.plot(param_range, val_mean, 'o-', color='red', label='Validation score')\n",
    "    plt.fill_between(param_range, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('F1 Score')\n",
    "    plt.title(f'Validation Curve - {title}')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "# ===================================================================\n",
    "# END OF THE CORRECTED CODE BLOCK\n",
    "# ===================================================================\n",
    "\n",
    "print(\"üìä Generating validation curves for key parameters...\")\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "n_estimators_range = [10, 50, 100, 150, 200, 300]\n",
    "plot_validation_curve(rf_model, X_train, y_train, 'n_estimators',\n",
    "                     n_estimators_range, 'Random Forest n_estimators')\n",
    "\n",
    "svm_model = SVC(random_state=42)\n",
    "c_range = [0.1, 1, 10, 100, 1000]\n",
    "plot_validation_curve(svm_model, X_train, y_train, 'C',\n",
    "                     c_range, 'SVM C parameter')\n",
    "\n",
    "# 8. FEATURE IMPORTANCE FROM BEST MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"8. FEATURE IMPORTANCE FROM OPTIMIZED MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset_name, data in datasets.items():\n",
    "    print(f\"\\nüìä Feature Importance - {dataset_name} Dataset:\")\n",
    "    tree_models = ['DecisionTree', 'RandomForest']\n",
    "    fig, axes = plt.subplots(1, len(tree_models), figsize=(12, 5))\n",
    "    if len(tree_models) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for i, model_name in enumerate(tree_models):\n",
    "        best_model = grid_search_results[dataset_name][model_name]['best_estimator']\n",
    "        feature_names = data['X_train'].columns\n",
    "        importances = best_model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "\n",
    "        axes[i].barh(range(len(importances)), importances[indices], alpha=0.7)\n",
    "        axes[i].set_yticks(range(len(importances)))\n",
    "        axes[i].set_yticklabels([feature_names[idx] for idx in indices])\n",
    "        axes[i].set_xlabel('Importance Score')\n",
    "        axes[i].set_title(f'Optimized {model_name}')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "        print(f\"  {model_name} - Top 5 features:\")\n",
    "        for j in range(min(5, len(indices))):\n",
    "            idx = indices[j]\n",
    "            print(f\"    {j+1}. {feature_names[idx]}: {importances[idx]:.4f}\")\n",
    "\n",
    "    plt.suptitle(f'Feature Importance - {dataset_name} Dataset', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 9. MODEL PERFORMANCE SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"9. FINAL MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "final_results_data = []\n",
    "for dataset_name in datasets.keys():\n",
    "    for model_name in models.keys():\n",
    "        cv_score = grid_search_results[dataset_name][model_name]['best_score']\n",
    "        test_acc = test_results[dataset_name][model_name]['accuracy']\n",
    "        test_f1 = test_results[dataset_name][model_name]['f1_score']\n",
    "        test_auc = test_results[dataset_name][model_name]['roc_auc']\n",
    "        final_results_data.append([\n",
    "            dataset_name, model_name, cv_score, test_acc, test_f1, test_auc\n",
    "        ])\n",
    "final_results_df = pd.DataFrame(final_results_data,\n",
    "                               columns=['Dataset', 'Model', 'CV_F1', 'Test_Accuracy', 'Test_F1', 'Test_AUC'])\n",
    "print(\"üèÜ Final Model Performance Summary:\")\n",
    "print(final_results_df.round(4))\n",
    "\n",
    "best_model_idx = final_results_df['Test_F1'].idxmax()\n",
    "best_model_info = final_results_df.loc[best_model_idx]\n",
    "\n",
    "print(f\"\\nü•á Overall Best Model:\")\n",
    "print(f\"  Dataset: {best_model_info['Dataset']}\")\n",
    "print(f\"  Model: {best_model_info['Model']}\")\n",
    "print(f\"  Test F1 Score: {best_model_info['Test_F1']:.4f}\")\n",
    "print(f\"  Test Accuracy: {best_model_info['Test_Accuracy']:.4f}\")\n",
    "print(f\"  Test AUC: {best_model_info['Test_AUC']:.4f}\")\n",
    "\n",
    "# 10. DETAILED CLASSIFICATION REPORTS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"10. DETAILED CLASSIFICATION REPORTS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for dataset_name in datasets.keys():\n",
    "    print(f\"\\nüìã Classification Reports - {dataset_name} Dataset:\")\n",
    "    for model_name in models.keys():\n",
    "        y_pred = test_results[dataset_name][model_name]['y_pred']\n",
    "        print(f\"\\n{model_name} (Optimized):\")\n",
    "        print(classification_report(y_test, y_pred,\n",
    "                                  target_names=['No Disease', 'Disease']))\n",
    "\n",
    "# 11. SAVE FINAL OPTIMIZED MODELS\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"11. SAVING OPTIMIZED MODELS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    os.makedirs('../results', exist_ok=True)\n",
    "    best_models_info = {}\n",
    "    for dataset_name in datasets.keys():\n",
    "        dataset_results = final_results_df[final_results_df['Dataset'] == dataset_name]\n",
    "        best_idx = dataset_results['Test_F1'].idxmax()\n",
    "        best_model_row = final_results_df.loc[best_idx]\n",
    "        best_model_name = best_model_row['Model']\n",
    "        best_model = grid_search_results[dataset_name][best_model_name]['best_estimator']\n",
    "        best_params = grid_search_results[dataset_name][best_model_name]['best_params']\n",
    "        model_filename = f'final_optimized_model_{dataset_name.lower()}.pkl'\n",
    "        joblib.dump(best_model, f'../models/{model_filename}')\n",
    "\n",
    "        best_models_info[dataset_name] = {\n",
    "            'model_name': best_model_name,\n",
    "            'best_params': best_params,\n",
    "            'cv_f1_score': float(best_model_row['CV_F1']),\n",
    "            'test_accuracy': float(best_model_row['Test_Accuracy']),\n",
    "            'test_f1_score': float(best_model_row['Test_F1']),\n",
    "            'test_auc': float(best_model_row['Test_AUC']),\n",
    "            'filename': model_filename,\n",
    "            'features': list(datasets[dataset_name]['X_train'].columns)\n",
    "        }\n",
    "        print(f\"‚úÖ Saved optimized {best_model_name} for {dataset_name}\")\n",
    "\n",
    "    final_results_df.to_csv('results/hyperparameter_tuning_results.csv', index=False)\n",
    "    comparison_df.to_csv('results/optimization_methods_comparison.csv', index=False)\n",
    "\n",
    "    with open('../results/final_optimized_models_info.json', 'w') as f:\n",
    "        json.dump(best_models_info, f, indent=2)\n",
    "\n",
    "    hyperparameter_results = {}\n",
    "    for dataset_name in datasets.keys():\n",
    "        hyperparameter_results[dataset_name] = {}\n",
    "        for model_name in models.keys():\n",
    "            hyperparameter_results[dataset_name][model_name] = {\n",
    "                'best_params': grid_search_results[dataset_name][model_name]['best_params'],\n",
    "                'best_cv_score': float(grid_search_results[dataset_name][model_name]['best_score'])\n",
    "            }\n",
    "    with open('../results/best_hyperparameters.json', 'w') as f:\n",
    "        json.dump(hyperparameter_results, f, indent=2)\n",
    "\n",
    "    print(f\"\\n‚úÖ All results saved successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error saving files: {e}\")\n",
    "\n",
    "# 12. HYPERPARAMETER TUNING SUMMARY\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"12. HYPERPARAMETER TUNING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"‚úÖ Hyperparameter tuning completed successfully!\")\n",
    "print(f\"üìä Models optimized: {len(models)}\")\n",
    "print(f\"üìä Datasets used: {len(datasets)}\")\n",
    "print(f\"üìä Optimization methods: GridSearchCV, RandomizedSearchCV\")\n",
    "\n",
    "print(f\"\\nüèÜ Best optimized models:\")\n",
    "for dataset_name, info in best_models_info.items():\n",
    "    print(f\"  {dataset_name}: {info['model_name']}\")\n",
    "    print(f\"    Test F1: {info['test_f1_score']:.4f}\")\n",
    "    print(f\"    Test Accuracy: {info['test_accuracy']:.4f}\")\n",
    "    print(f\"    Best params: {info['best_params']}\")\n",
    "\n",
    "print(f\"\\nüéØ Project completion status:\")\n",
    "print(\"  1. ‚úÖ Data preprocessing complete\")\n",
    "print(\"  2. ‚úÖ PCA analysis complete\")\n",
    "print(\"  3. ‚úÖ Feature selection complete\")\n",
    "print(\"  4. ‚úÖ Supervised learning complete\")\n",
    "print(\"  5. ‚úÖ Unsupervised learning complete\")\n",
    "print(\"  6. ‚úÖ Hyperparameter tuning complete\")\n",
    "\n",
    "print(f\"\\nüéâ Machine Learning Pipeline Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
